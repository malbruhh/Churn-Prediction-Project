{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5203754a",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction: Neural Network Implementation\n",
    "This implements a custom Neural Network from scratch using NumPy to predict customer churn. It includes a complete preprocessing pipeline featuring log transformations, one-hot encoding, and SMOTE for handling class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e1ab9c",
   "metadata": {},
   "source": [
    "## Steps:\n",
    "1. Import Libraries \n",
    "2. Read Data from `\\Dataset` folder (Retrieved from UCI Repositories)\n",
    "3. Clean Dataset (rename and format the column)\n",
    "4. Data Preprocessing\n",
    "5. Initialize the model\n",
    "6. Train and test the data\n",
    "7. Evaluate Performance using graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa48bd7",
   "metadata": {},
   "source": [
    "### 0. Install Library Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a5eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas matplotlib seaborn tabulate imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b92ecb",
   "metadata": {},
   "source": [
    "### 1. Import Libraries\n",
    "Import necessary libraries:\n",
    "* `numpy`     : High performance array object\n",
    "* `pandas`    : Data analysis and manipulation tools\n",
    "* `matplotlib`: Data visualizations\n",
    "* `seaborn`   : Data visualizations framework based on **matplotlib**\n",
    "* `tabulate`  : Pretty-print tabular structure library\n",
    "* `imblearn`  : SMOTE class balance\n",
    "\n",
    "#### 1.1 Initialize variables\n",
    "* `EPOCHS`      : Iteration counts\n",
    "* `BATCH_SIZE`  : Hyperparameter of size of batch for smaller subsets\n",
    "* `PATIENCE`    : Hyperparameter stopping condition that wait for an improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150efdc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828c5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import seaborn as sns\n",
    "\n",
    "EPOCHS = 250\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d1323f",
   "metadata": {},
   "source": [
    "#### 1.2 Helper Functions\n",
    "1.1 Handle mathematical operation used in the `NeuralNetwork` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a4f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): return 1 /(1 + np.exp(-x))\n",
    "def derivative_sigmoid(sigmoid_x): return sigmoid_x * (1- sigmoid_x )\n",
    "def relu(x): return np.maximum(0,x)\n",
    "def derivative_relu(relu_x): return (relu_x>0).astype(float)\n",
    "def binary_cross_entropy(y_true, y_pred, epsilon=1e-15):\n",
    "    # Clip predictions to prevent log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b0b083",
   "metadata": {},
   "source": [
    "### 2. Read and Display Dataset Information\n",
    "load the dataset and understand the data, and use the Interquartile Range (IQR) method to identify potential outliers in numerical columns.\n",
    "2.1 Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02cac61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path: str):\n",
    "    df = pd.read_csv(file_path)\n",
    "    new_columns = [col.strip().replace('  ', ' ').replace(' ', '_').lower() for col in df.columns]\n",
    "    df.columns = new_columns\n",
    "    return df\n",
    "\n",
    "path = r'..\\Dataset\\Customer Churn.csv'\n",
    "df = read_file(path)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f83f707",
   "metadata": {},
   "source": [
    "2.2 Display dataset data types and null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cfaf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aaa837",
   "metadata": {},
   "source": [
    "2.3 Display dataset shape and its statisical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cce58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc190a3",
   "metadata": {},
   "source": [
    "2.4. Detect Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a56f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outlier summary (IQR method):\n",
      "Column                     Outlier Count    Lower Bound    Upper Bound  Percentage\n",
      "-----------------------  ---------------  -------------  -------------  ------------\n",
      "call_failure                          47         -15.5           28.5   1.49%\n",
      "subscription_length                  282          18             50     8.95%\n",
      "charge_amount                        370          -1.5            2.5   11.75%\n",
      "seconds_of_use                       200       -6239.25       14108.8   6.35%\n",
      "frequency_of_use                     129         -75            197     4.10%\n",
      "frequency_of_sms                     368        -115.5          208.5   11.68%\n",
      "distinct_called_numbers               77         -26             70     2.44%\n",
      "age_group                            170           0.5            4.5   5.40%\n",
      "age                                  688          17.5           37.5   21.84%\n",
      "customer_value                       116        -898.08        1800.27  3.68%\n"
     ]
    }
   ],
   "source": [
    "def detect_outliers_iqr(df, k=1.5):\n",
    "    nums = df.select_dtypes(include='number')\n",
    "    outlier_info = {}\n",
    "    \n",
    "    for c in nums.columns:\n",
    "        #skip binary columns\n",
    "        if(nums[c].nunique() <= 2):\n",
    "            continue\n",
    "        \n",
    "        q1 = nums[c].quantile(0.25) #1st quartile\n",
    "        q3 = nums[c].quantile(0.75) #3rd quartile\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - k * iqr\n",
    "        upper = q3 + k * iqr\n",
    "        mask = (nums[c] < lower) | (nums[c] > upper)\n",
    "        outlier_info[c] = {\n",
    "            'count': int(mask.sum()),\n",
    "            'indices': nums.index[mask].tolist(),\n",
    "            'lower': float(lower),\n",
    "            'upper': float(upper)\n",
    "        }\n",
    "    return outlier_info\n",
    "\n",
    "outliers = detect_outliers_iqr(df)\n",
    "table_data = []\n",
    "print('\\nOutlier summary (IQR method):')\n",
    "for col, info in outliers.items():\n",
    "    if info['count'] > 0:\n",
    "    # Calculate percentage\n",
    "        perc = (info[\"count\"] / len(df)) * 100\n",
    "        \n",
    "        # Add a list (row) to our table_data\n",
    "        table_data.append([col, info[\"count\"], f\"{info['lower']:.3f}\",\n",
    "            f\"{info['upper']:.3f}\", f\"{perc:.2f}%\"])\n",
    "headers = [\"Column\", \"Outlier Count\", \"Lower Bound\", \"Upper Bound\", \"Percentage\"]\n",
    "print(tabulate(table_data, headers=headers))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f06022",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning \n",
    "This process take step before handling with outliers and data preprocessing.\n",
    "\n",
    "3.1 Remove Duplicate Row (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "print(f'\\n[Changes] Removed duplicate rows. New shape={df.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c61f70f",
   "metadata": {},
   "source": [
    "3.2 Remove Redundant Groups\n",
    "\n",
    "`age_group` and `age` columns both have the same values but in different types, numeric and nominal respectively.\n",
    "`age_group` column is dropped to prevent biases when learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54814c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['age_group'])\n",
    "print(f'\\n[Changes] Dropped column: age_group due to redundancy. New shape={df.shape}\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128d4f3",
   "metadata": {},
   "source": [
    "### 4. Data Preprocessing\n",
    "\n",
    "The dataset will follows the exact steps to avoid any imbalance or data leakage during testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5594373b",
   "metadata": {},
   "source": [
    "[Split] -> [Log] -> [Encode] -> [Fit] -> [Scale] -> [Balance]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff70aca",
   "metadata": {},
   "source": [
    "#### 4.1 Split\n",
    "\n",
    "* Splitting the dataset into train, and test sets, and both input,X and output,Y\n",
    "* This function will reset the intialized index of X and Y dataset, and shuffle them before splitting to prevent **data leakage**\n",
    "\n",
    "@function `split_data()`:\n",
    "* params\n",
    "    * `X`, `y` as input and output\n",
    "    * `test_split` as testing split percentage ; default `0.2`\n",
    "    * `randomness` as random values for randomize dataset index before splitting ; default `None`\n",
    "* return\n",
    "    * `X_train` = Training set without `Churn`\n",
    "    * `Y_train` = `Churn` Training column\n",
    "    * `Y_train` = Testing set without `Churn`\n",
    "    * `Y_test` = `Churn` Testing column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63342842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, test_split=0.2, randomness=None):\n",
    "    # Set seed for reproducibility\n",
    "    if randomness is not None:\n",
    "        np.random.seed(randomness)\n",
    "    \n",
    "    # reset X and Y current index\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    \n",
    "    # Identify unique classes and their indices (0 and 1)\n",
    "    unique_classes = np.unique(y)\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    for cls in unique_classes:\n",
    "        # Get indices of rows belonging to this class\n",
    "        cls_indices = np.where(y == cls)[0]\n",
    "\n",
    "        # Shuffle indices within this specific class\n",
    "        np.random.shuffle(cls_indices)\n",
    "\n",
    "        # Determine the split point\n",
    "        total_count = len(cls_indices)\n",
    "        test_count = int(total_count * test_split)\n",
    "        \n",
    "        # Split indices\n",
    "        cls_test = cls_indices[:test_count]\n",
    "        cls_train = cls_indices[test_count:]\n",
    "        \n",
    "        # Add to main lists\n",
    "        test_indices.extend(cls_test)\n",
    "        train_indices.extend(cls_train)\n",
    "        \n",
    "    # Shuffle the final combined indices so they aren't grouped by class\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "    \n",
    "    # Use .iloc for DataFrames to select the rows\n",
    "    X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
    "    y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X = df.drop(columns=['churn'], axis=1)\n",
    "Y = df['churn']\n",
    "X_train,X_test,y_train,y_test = split_data(X,Y,test_split=0.2, randomness=42)\n",
    "print(f'[Changes] Successfully split data into Training and Testing.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6033f79",
   "metadata": {},
   "source": [
    "#### 4.2 Log Transformation\n",
    "\n",
    "* This will treat the outliers found before, compressing all the values within the dataset to **reduce** the skewness of the data\n",
    "* This step is to prevent unfair patterns.\n",
    "* Use `numpy.log(1 + x)`\n",
    "* log transformation on seperated train and test set to prevent **data leakage**\n",
    "\n",
    "@function `log_transformation()`:\n",
    "* params\n",
    "    * `df_train`, `df_test` as training and testing dataset\n",
    "    * `cols_log` as list of column to be log transformed\n",
    "* return\n",
    "    * `df_train` = train dataset that have been log transformed\n",
    "    * `df_test` = test dataset that have been log transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80bdda42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Changes] Applied log transformation to selected columns.\n"
     ]
    }
   ],
   "source": [
    "def log_transformation(df_train, df_test, cols_log: list):\n",
    "    for col in cols_log:\n",
    "        df_train[col] = np.log1p(df_train[col])\n",
    "        df_test[col] = np.log1p(df_test[col])\n",
    "    print(f'[Changes] Applied log transformation to selected columns.')\n",
    "    return df_train, df_test\n",
    "\n",
    "cols_to_log = [\n",
    "    'seconds_of_use',\n",
    "    'frequency_of_use',\n",
    "    'frequency_of_sms',\n",
    "    'distinct_called_numbers',\n",
    "    'call_failure',\n",
    "    'customer_value',\n",
    "    'charge_amount'\n",
    "]\n",
    "X_train, X_test = log_transformation(X_train,X_test,cols_to_log)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831f2b00",
   "metadata": {},
   "source": [
    "#### 4.3 One-Hot Encoding\n",
    "\n",
    "* Convert categorical column into multiple, numeric dummy columns that values between `0` and `1`.\n",
    "* Example: `plan` have 3 categories  = `[cat1, cat2, cat3]`. It will split into dummy columns for each category, `plan_cat1`, `plan_cat2`, `plan_cat3` \n",
    "* use drop first column attribute to prevent **multicollinearity**\n",
    "\n",
    "@function `get_train_category()`:\n",
    "* params\n",
    "    * `df` as dataset\n",
    "    * `col_name` retrieve category from this column\n",
    "*return\n",
    "    * `list` = list of column name\n",
    "\n",
    "\n",
    "@function `one_hot_encoding()`:\n",
    "* params\n",
    "    * `df` as dataset\n",
    "    * `col_name` as column name that will apply one-hot encoding\n",
    "    * `categories` as category inside `col_name`\n",
    "    * `drop_first` drop first column to simplify the dummy column\n",
    "* return\n",
    "    * `converted_pd` = return the dataframe with the added dummy columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d582d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
